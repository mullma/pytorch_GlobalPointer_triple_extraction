2022-06-29 11:48:12,004 - INFO - main.py - <module> - 315 - Namespace(adam_epsilon=1e-08, bert_dir='model_hub/chinese-bert-wwm-ext/', data_dir='./data/ske', dropout_prob=0.1, eval_batch_size=8, eval_steps=100, gpu_ids='0', log_dir='./logs/', lr=5e-05, max_grad_norm=1.0, max_seq_len=256, num_tags=49, other_lr=5e-05, output_dir='./checkpoints/', seed=123, train_batch_size=32, train_epochs=1, use_dev_num=1000, use_tensorboard='False', warmup_proportion=0.1, weight_decay=0.01)
2022-06-29 11:49:58,527 - INFO - main.py - <module> - 315 - Namespace(adam_epsilon=1e-08, bert_dir='model_hub/chinese-bert-wwm-ext/', data_dir='./data/ske', dropout_prob=0.1, eval_batch_size=8, eval_steps=100, gpu_ids='0', log_dir='./logs/', lr=5e-05, max_grad_norm=1.0, max_seq_len=256, num_tags=49, other_lr=5e-05, output_dir='./checkpoints/', seed=123, train_batch_size=32, train_epochs=1, use_dev_num=1000, use_tensorboard='False', warmup_proportion=0.1, weight_decay=0.01)
2022-06-29 11:50:31,279 - INFO - main.py - <module> - 315 - Namespace(adam_epsilon=1e-08, bert_dir='model_hub/chinese-bert-wwm-ext/', data_dir='./data/ske', dropout_prob=0.1, eval_batch_size=8, eval_steps=100, gpu_ids='0', log_dir='./logs/', lr=5e-05, max_grad_norm=1.0, max_seq_len=256, num_tags=49, other_lr=5e-05, output_dir='./checkpoints/', seed=123, train_batch_size=32, train_epochs=1, use_dev_num=1000, use_tensorboard='False', warmup_proportion=0.1, weight_decay=0.01)
2022-06-29 11:50:47,094 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['0']
2022-06-29 11:51:29,678 - INFO - main.py - <module> - 315 - Namespace(adam_epsilon=1e-08, bert_dir='model_hub/chinese-bert-wwm-ext/', data_dir='./data/ske', dropout_prob=0.1, eval_batch_size=8, eval_steps=100, gpu_ids='0', log_dir='./logs/', lr=5e-05, max_grad_norm=1.0, max_seq_len=256, num_tags=49, other_lr=5e-05, output_dir='./checkpoints/', seed=123, train_batch_size=32, train_epochs=1, use_dev_num=1000, use_tensorboard='False', warmup_proportion=0.1, weight_decay=0.01)
2022-06-29 11:51:36,701 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['0']
2022-06-30 02:11:39,922 - INFO - main.py - <module> - 294 - Namespace(adam_epsilon=1e-08, bert_dir='model_hub/chinese-bert-wwm-ext/', data_dir='./data/ske', dropout_prob=0.1, eval_batch_size=8, eval_steps=100, gpu_ids='0', log_dir='./logs/', lr=5e-05, max_grad_norm=1.0, max_seq_len=256, num_tags=49, other_lr=5e-05, output_dir='./checkpoints/', seed=123, train_batch_size=32, train_epochs=1, use_dev_num=1000, use_tensorboard='False', warmup_proportion=0.1, weight_decay=0.01)
2022-06-30 02:12:03,590 - INFO - main.py - <module> - 294 - Namespace(adam_epsilon=1e-08, bert_dir='model_hub/chinese-bert-wwm-ext/', data_dir='./data/ske', dropout_prob=0.1, eval_batch_size=8, eval_steps=100, gpu_ids='0', log_dir='./logs/', lr=5e-05, max_grad_norm=1.0, max_seq_len=256, num_tags=49, other_lr=5e-05, output_dir='./checkpoints/', seed=123, train_batch_size=32, train_epochs=1, use_dev_num=1000, use_tensorboard='False', warmup_proportion=0.1, weight_decay=0.01)
2022-06-30 02:12:30,170 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['0']
2022-06-30 02:12:41,438 - INFO - train_utils.py - load_model_and_parallel - 87 - Load ckpt from ./checkpoints/bert/model.pt
2022-06-30 02:12:42,516 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['0']
2022-06-30 02:26:07,338 - INFO - main.py - <module> - 245 - Namespace(adam_epsilon=1e-08, bert_dir='model_hub/chinese-bert-wwm-ext/', data_dir='./data/ske', dropout_prob=0.1, eval_batch_size=8, eval_steps=100, gpu_ids='0', log_dir='./logs/', lr=5e-05, max_grad_norm=1.0, max_seq_len=256, num_tags=49, other_lr=5e-05, output_dir='./checkpoints/', seed=123, train_batch_size=32, train_epochs=1, use_dev_num=1000, use_tensorboard='False', warmup_proportion=0.1, weight_decay=0.01)
2022-06-30 02:26:14,214 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['0']
2022-06-30 02:26:22,612 - INFO - train_utils.py - load_model_and_parallel - 87 - Load ckpt from ./checkpoints/bert/model.pt
2022-06-30 02:26:23,623 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['0']
2022-06-30 02:27:46,714 - INFO - main.py - <module> - 245 - Namespace(adam_epsilon=1e-08, bert_dir='model_hub/chinese-bert-wwm-ext/', data_dir='./data/ske', dropout_prob=0.1, eval_batch_size=8, eval_steps=100, gpu_ids='0', log_dir='./logs/', lr=5e-05, max_grad_norm=1.0, max_seq_len=256, num_tags=49, other_lr=5e-05, output_dir='./checkpoints/', seed=123, train_batch_size=32, train_epochs=1, use_dev_num=1000, use_tensorboard='False', warmup_proportion=0.1, weight_decay=0.01)
2022-06-30 02:27:53,669 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['0']
2022-06-30 02:28:02,258 - INFO - train_utils.py - load_model_and_parallel - 87 - Load ckpt from ./checkpoints/bert/model.pt
2022-06-30 02:28:03,191 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['0']
2022-06-30 02:28:52,891 - INFO - main.py - <module> - 245 - Namespace(adam_epsilon=1e-08, bert_dir='model_hub/chinese-bert-wwm-ext/', data_dir='./data/ske', dropout_prob=0.1, eval_batch_size=8, eval_steps=100, gpu_ids='0', log_dir='./logs/', lr=5e-05, max_grad_norm=1.0, max_seq_len=256, num_tags=49, other_lr=5e-05, output_dir='./checkpoints/', seed=123, train_batch_size=32, train_epochs=1, use_dev_num=1000, use_tensorboard='False', warmup_proportion=0.1, weight_decay=0.01)
2022-06-30 02:28:59,734 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['0']
2022-06-30 02:29:08,295 - INFO - train_utils.py - load_model_and_parallel - 87 - Load ckpt from ./checkpoints/bert/model.pt
2022-06-30 02:29:09,217 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['0']
2022-06-30 02:32:00,288 - INFO - main.py - <module> - 247 - Namespace(adam_epsilon=1e-08, bert_dir='model_hub/chinese-bert-wwm-ext/', data_dir='./data/ske', dropout_prob=0.1, eval_batch_size=8, eval_steps=100, gpu_ids='0', log_dir='./logs/', lr=5e-05, max_grad_norm=1.0, max_seq_len=256, num_tags=49, other_lr=5e-05, output_dir='./checkpoints/', seed=123, train_batch_size=32, train_epochs=1, use_dev_num=1000, use_tensorboard='False', warmup_proportion=0.1, weight_decay=0.01)
2022-06-30 02:32:07,429 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['0']
2022-06-30 02:32:16,005 - INFO - train_utils.py - load_model_and_parallel - 87 - Load ckpt from ./checkpoints/bert/model.pt
2022-06-30 02:32:17,131 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['0']
2022-06-30 02:32:59,456 - INFO - main.py - <module> - 247 - Namespace(adam_epsilon=1e-08, bert_dir='model_hub/chinese-bert-wwm-ext/', data_dir='./data/ske', dropout_prob=0.1, eval_batch_size=8, eval_steps=100, gpu_ids='0', log_dir='./logs/', lr=5e-05, max_grad_norm=1.0, max_seq_len=256, num_tags=49, other_lr=5e-05, output_dir='./checkpoints/', seed=123, train_batch_size=32, train_epochs=1, use_dev_num=1000, use_tensorboard='False', warmup_proportion=0.1, weight_decay=0.01)
2022-06-30 02:33:06,349 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['0']
2022-06-30 02:33:14,933 - INFO - train_utils.py - load_model_and_parallel - 87 - Load ckpt from ./checkpoints/bert/model.pt
2022-06-30 02:33:15,858 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['0']
2022-06-30 02:35:54,312 - INFO - main.py - <module> - 246 - Namespace(adam_epsilon=1e-08, bert_dir='model_hub/chinese-bert-wwm-ext/', data_dir='./data/ske', dropout_prob=0.1, eval_batch_size=8, eval_steps=100, gpu_ids='0', log_dir='./logs/', lr=5e-05, max_grad_norm=1.0, max_seq_len=256, num_tags=49, other_lr=5e-05, output_dir='./checkpoints/', seed=123, train_batch_size=32, train_epochs=1, use_dev_num=1000, use_tensorboard='False', warmup_proportion=0.1, weight_decay=0.01)
2022-06-30 02:36:01,613 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['0']
2022-06-30 02:36:10,140 - INFO - train_utils.py - load_model_and_parallel - 87 - Load ckpt from ./checkpoints/bert/model.pt
2022-06-30 02:36:11,075 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['0']
2022-06-30 02:36:43,697 - INFO - main.py - <module> - 247 - Namespace(adam_epsilon=1e-08, bert_dir='model_hub/chinese-bert-wwm-ext/', data_dir='./data/ske', dropout_prob=0.1, eval_batch_size=8, eval_steps=100, gpu_ids='0', log_dir='./logs/', lr=5e-05, max_grad_norm=1.0, max_seq_len=256, num_tags=49, other_lr=5e-05, output_dir='./checkpoints/', seed=123, train_batch_size=32, train_epochs=1, use_dev_num=1000, use_tensorboard='False', warmup_proportion=0.1, weight_decay=0.01)
2022-06-30 02:36:50,572 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['0']
2022-06-30 02:36:59,058 - INFO - train_utils.py - load_model_and_parallel - 87 - Load ckpt from ./checkpoints/bert/model.pt
2022-06-30 02:36:59,988 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['0']
2022-06-30 02:38:37,012 - INFO - main.py - <module> - 248 - Namespace(adam_epsilon=1e-08, bert_dir='model_hub/chinese-bert-wwm-ext/', data_dir='./data/ske', dropout_prob=0.1, eval_batch_size=8, eval_steps=100, gpu_ids='0', log_dir='./logs/', lr=5e-05, max_grad_norm=1.0, max_seq_len=256, num_tags=49, other_lr=5e-05, output_dir='./checkpoints/', seed=123, train_batch_size=32, train_epochs=1, use_dev_num=1000, use_tensorboard='False', warmup_proportion=0.1, weight_decay=0.01)
2022-06-30 02:38:43,990 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['0']
2022-06-30 02:38:52,993 - INFO - train_utils.py - load_model_and_parallel - 87 - Load ckpt from ./checkpoints/bert/model.pt
2022-06-30 02:38:54,328 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['0']
2022-06-30 02:44:44,645 - INFO - main.py - <module> - 258 - Namespace(adam_epsilon=1e-08, bert_dir='model_hub/chinese-bert-wwm-ext/', data_dir='./data/ske', dropout_prob=0.1, eval_batch_size=8, eval_steps=100, gpu_ids='0', log_dir='./logs/', lr=5e-05, max_grad_norm=1.0, max_seq_len=256, num_tags=49, other_lr=5e-05, output_dir='./checkpoints/', seed=123, train_batch_size=32, train_epochs=1, use_dev_num=1000, use_tensorboard='False', warmup_proportion=0.1, weight_decay=0.01)
2022-06-30 02:44:51,599 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['0']
2022-06-30 02:45:00,068 - INFO - train_utils.py - load_model_and_parallel - 87 - Load ckpt from ./checkpoints/bert/model.pt
2022-06-30 02:45:01,028 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['0']
2022-06-30 02:45:54,371 - INFO - main.py - <module> - 258 - Namespace(adam_epsilon=1e-08, bert_dir='model_hub/chinese-bert-wwm-ext/', data_dir='./data/ske', dropout_prob=0.1, eval_batch_size=8, eval_steps=100, gpu_ids='0', log_dir='./logs/', lr=5e-05, max_grad_norm=1.0, max_seq_len=256, num_tags=49, other_lr=5e-05, output_dir='./checkpoints/', seed=123, train_batch_size=32, train_epochs=1, use_dev_num=1000, use_tensorboard='False', warmup_proportion=0.1, weight_decay=0.01)
2022-06-30 02:46:01,411 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['0']
2022-06-30 02:46:09,887 - INFO - train_utils.py - load_model_and_parallel - 87 - Load ckpt from ./checkpoints/bert/model.pt
2022-06-30 02:46:10,848 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['0']
2022-06-30 02:47:06,798 - INFO - main.py - <module> - 259 - Namespace(adam_epsilon=1e-08, bert_dir='model_hub/chinese-bert-wwm-ext/', data_dir='./data/ske', dropout_prob=0.1, eval_batch_size=8, eval_steps=100, gpu_ids='0', log_dir='./logs/', lr=5e-05, max_grad_norm=1.0, max_seq_len=256, num_tags=49, other_lr=5e-05, output_dir='./checkpoints/', seed=123, train_batch_size=32, train_epochs=1, use_dev_num=1000, use_tensorboard='False', warmup_proportion=0.1, weight_decay=0.01)
2022-06-30 02:47:14,152 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['0']
2022-06-30 02:47:23,172 - INFO - train_utils.py - load_model_and_parallel - 87 - Load ckpt from ./checkpoints/bert/model.pt
2022-06-30 02:47:24,137 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['0']
2022-06-30 02:50:17,231 - INFO - main.py - <module> - 261 - Namespace(adam_epsilon=1e-08, bert_dir='model_hub/chinese-bert-wwm-ext/', data_dir='./data/ske', dropout_prob=0.1, eval_batch_size=8, eval_steps=100, gpu_ids='0', log_dir='./logs/', lr=5e-05, max_grad_norm=1.0, max_seq_len=256, num_tags=49, other_lr=5e-05, output_dir='./checkpoints/', seed=123, train_batch_size=32, train_epochs=1, use_dev_num=1000, use_tensorboard='False', warmup_proportion=0.1, weight_decay=0.01)
2022-06-30 02:50:24,204 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['0']
2022-06-30 02:50:32,708 - INFO - train_utils.py - load_model_and_parallel - 87 - Load ckpt from ./checkpoints/bert/model.pt
2022-06-30 02:50:33,671 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['0']
2022-06-30 02:52:15,179 - INFO - main.py - <module> - 266 - Namespace(adam_epsilon=1e-08, bert_dir='model_hub/chinese-bert-wwm-ext/', data_dir='./data/ske', dropout_prob=0.1, eval_batch_size=8, eval_steps=100, gpu_ids='0', log_dir='./logs/', lr=5e-05, max_grad_norm=1.0, max_seq_len=256, num_tags=49, other_lr=5e-05, output_dir='./checkpoints/', seed=123, train_batch_size=32, train_epochs=1, use_dev_num=1000, use_tensorboard='False', warmup_proportion=0.1, weight_decay=0.01)
2022-06-30 02:52:22,175 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['0']
2022-06-30 02:52:30,733 - INFO - train_utils.py - load_model_and_parallel - 87 - Load ckpt from ./checkpoints/bert/model.pt
2022-06-30 02:52:31,705 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['0']
2022-06-30 02:55:04,553 - INFO - main.py - <module> - 267 - Namespace(adam_epsilon=1e-08, bert_dir='model_hub/chinese-bert-wwm-ext/', data_dir='./data/ske', dropout_prob=0.1, eval_batch_size=8, eval_steps=100, gpu_ids='0', log_dir='./logs/', lr=5e-05, max_grad_norm=1.0, max_seq_len=256, num_tags=49, other_lr=5e-05, output_dir='./checkpoints/', seed=123, train_batch_size=32, train_epochs=1, use_dev_num=1000, use_tensorboard='False', warmup_proportion=0.1, weight_decay=0.01)
2022-06-30 02:55:12,311 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['0']
2022-06-30 02:55:21,395 - INFO - train_utils.py - load_model_and_parallel - 87 - Load ckpt from ./checkpoints/bert/model.pt
2022-06-30 02:55:22,548 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['0']
2022-06-30 02:57:29,336 - INFO - main.py - <module> - 281 - Namespace(adam_epsilon=1e-08, bert_dir='model_hub/chinese-bert-wwm-ext/', data_dir='./data/ske', dropout_prob=0.1, eval_batch_size=8, eval_steps=100, gpu_ids='0', log_dir='./logs/', lr=5e-05, max_grad_norm=1.0, max_seq_len=256, num_tags=49, other_lr=5e-05, output_dir='./checkpoints/', seed=123, train_batch_size=32, train_epochs=1, use_dev_num=1000, use_tensorboard='False', warmup_proportion=0.1, weight_decay=0.01)
2022-06-30 02:57:36,520 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['0']
2022-06-30 02:57:45,085 - INFO - train_utils.py - load_model_and_parallel - 87 - Load ckpt from ./checkpoints/bert/model.pt
2022-06-30 02:57:46,024 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['0']
2022-06-30 02:58:51,474 - INFO - main.py - <module> - 281 - Namespace(adam_epsilon=1e-08, bert_dir='model_hub/chinese-bert-wwm-ext/', data_dir='./data/ske', dropout_prob=0.1, eval_batch_size=8, eval_steps=100, gpu_ids='0', log_dir='./logs/', lr=5e-05, max_grad_norm=1.0, max_seq_len=256, num_tags=49, other_lr=5e-05, output_dir='./checkpoints/', seed=123, train_batch_size=32, train_epochs=1, use_dev_num=1000, use_tensorboard='False', warmup_proportion=0.1, weight_decay=0.01)
2022-06-30 02:58:58,431 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['0']
2022-06-30 02:59:06,975 - INFO - train_utils.py - load_model_and_parallel - 87 - Load ckpt from ./checkpoints/bert/model.pt
2022-06-30 02:59:07,942 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['0']
2022-06-30 03:09:03,482 - INFO - main.py - <module> - 276 - Namespace(adam_epsilon=1e-08, bert_dir='model_hub/chinese-bert-wwm-ext/', data_dir='./data/ske', dropout_prob=0.1, eval_batch_size=8, eval_steps=100, gpu_ids='0', log_dir='./logs/', lr=5e-05, max_grad_norm=1.0, max_seq_len=256, num_tags=49, other_lr=5e-05, output_dir='./checkpoints/', seed=123, train_batch_size=32, train_epochs=1, use_dev_num=1000, use_tensorboard='False', warmup_proportion=0.1, weight_decay=0.01)
2022-06-30 03:09:10,471 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['0']
2022-06-30 03:09:19,054 - INFO - train_utils.py - load_model_and_parallel - 87 - Load ckpt from ./checkpoints/bert/model.pt
2022-06-30 03:09:20,034 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['0']
2022-06-30 03:10:58,013 - INFO - main.py - <module> - 276 - Namespace(adam_epsilon=1e-08, bert_dir='model_hub/chinese-bert-wwm-ext/', data_dir='./data/ske', dropout_prob=0.1, eval_batch_size=8, eval_steps=100, gpu_ids='0', log_dir='./logs/', lr=5e-05, max_grad_norm=1.0, max_seq_len=256, num_tags=49, other_lr=5e-05, output_dir='./checkpoints/', seed=123, train_batch_size=32, train_epochs=1, use_dev_num=1000, use_tensorboard='False', warmup_proportion=0.1, weight_decay=0.01)
2022-06-30 03:11:04,963 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['0']
2022-06-30 03:11:13,499 - INFO - train_utils.py - load_model_and_parallel - 87 - Load ckpt from ./checkpoints/bert/model.pt
2022-06-30 03:11:14,463 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['0']
2022-06-30 03:14:59,353 - INFO - main.py - <module> - 279 - Namespace(adam_epsilon=1e-08, bert_dir='model_hub/chinese-bert-wwm-ext/', data_dir='./data/ske', dropout_prob=0.1, eval_batch_size=8, eval_steps=100, gpu_ids='0', log_dir='./logs/', lr=5e-05, max_grad_norm=1.0, max_seq_len=256, num_tags=49, other_lr=5e-05, output_dir='./checkpoints/', seed=123, train_batch_size=32, train_epochs=1, use_dev_num=1000, use_tensorboard='False', warmup_proportion=0.1, weight_decay=0.01)
2022-06-30 03:15:06,319 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['0']
2022-06-30 03:15:14,828 - INFO - train_utils.py - load_model_and_parallel - 87 - Load ckpt from ./checkpoints/bert/model.pt
2022-06-30 03:15:15,757 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['0']
2022-06-30 03:17:16,243 - INFO - main.py - <module> - 279 - Namespace(adam_epsilon=1e-08, bert_dir='model_hub/chinese-bert-wwm-ext/', data_dir='./data/ske', dropout_prob=0.1, eval_batch_size=8, eval_steps=100, gpu_ids='0', log_dir='./logs/', lr=5e-05, max_grad_norm=1.0, max_seq_len=256, num_tags=49, other_lr=5e-05, output_dir='./checkpoints/', seed=123, train_batch_size=32, train_epochs=1, use_dev_num=1000, use_tensorboard='False', warmup_proportion=0.1, weight_decay=0.01)
2022-06-30 03:17:23,241 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['0']
2022-06-30 03:17:31,867 - INFO - train_utils.py - load_model_and_parallel - 87 - Load ckpt from ./checkpoints/bert/model.pt
2022-06-30 03:17:32,805 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['0']
2022-06-30 03:17:44,900 - INFO - main.py - <module> - 279 - Namespace(adam_epsilon=1e-08, bert_dir='model_hub/chinese-bert-wwm-ext/', data_dir='./data/ske', dropout_prob=0.1, eval_batch_size=8, eval_steps=100, gpu_ids='0', log_dir='./logs/', lr=5e-05, max_grad_norm=1.0, max_seq_len=256, num_tags=49, other_lr=5e-05, output_dir='./checkpoints/', seed=123, train_batch_size=32, train_epochs=1, use_dev_num=1000, use_tensorboard='False', warmup_proportion=0.1, weight_decay=0.01)
2022-06-30 03:17:51,854 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['0']
2022-06-30 03:18:00,426 - INFO - train_utils.py - load_model_and_parallel - 87 - Load ckpt from ./checkpoints/bert/model.pt
2022-06-30 03:18:01,536 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['0']
2022-06-30 03:19:47,623 - INFO - main.py - <module> - 279 - Namespace(adam_epsilon=1e-08, bert_dir='model_hub/chinese-bert-wwm-ext/', data_dir='./data/ske', dropout_prob=0.1, eval_batch_size=8, eval_steps=100, gpu_ids='0', log_dir='./logs/', lr=5e-05, max_grad_norm=1.0, max_seq_len=256, num_tags=49, other_lr=5e-05, output_dir='./checkpoints/', seed=123, train_batch_size=32, train_epochs=1, use_dev_num=1000, use_tensorboard='False', warmup_proportion=0.1, weight_decay=0.01)
2022-06-30 03:19:54,624 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['0']
2022-06-30 03:20:03,068 - INFO - train_utils.py - load_model_and_parallel - 87 - Load ckpt from ./checkpoints/bert/model.pt
2022-06-30 03:20:04,020 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['0']
2022-06-30 03:24:38,076 - INFO - main.py - <module> - 312 - Namespace(adam_epsilon=1e-08, bert_dir='model_hub/chinese-bert-wwm-ext/', data_dir='./data/ske', dropout_prob=0.1, eval_batch_size=8, eval_steps=100, gpu_ids='0', log_dir='./logs/', lr=5e-05, max_grad_norm=1.0, max_seq_len=256, num_tags=49, other_lr=5e-05, output_dir='./checkpoints/', seed=123, train_batch_size=32, train_epochs=1, use_dev_num=1000, use_tensorboard='False', warmup_proportion=0.1, weight_decay=0.01)
2022-06-30 03:24:45,025 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['0']
2022-06-30 03:24:53,688 - INFO - train_utils.py - load_model_and_parallel - 87 - Load ckpt from ./checkpoints/bert/model.pt
2022-06-30 03:24:54,664 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['0']
2022-06-30 03:26:50,811 - INFO - main.py - <module> - 313 - Namespace(adam_epsilon=1e-08, bert_dir='model_hub/chinese-bert-wwm-ext/', data_dir='./data/ske', dropout_prob=0.1, eval_batch_size=8, eval_steps=100, gpu_ids='0', log_dir='./logs/', lr=5e-05, max_grad_norm=1.0, max_seq_len=256, num_tags=49, other_lr=5e-05, output_dir='./checkpoints/', seed=123, train_batch_size=32, train_epochs=1, use_dev_num=1000, use_tensorboard='False', warmup_proportion=0.1, weight_decay=0.01)
2022-06-30 03:26:57,934 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['0']
2022-06-30 03:27:06,607 - INFO - train_utils.py - load_model_and_parallel - 87 - Load ckpt from ./checkpoints/bert/model.pt
2022-06-30 03:27:07,579 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['0']
2022-06-30 03:30:37,398 - INFO - main.py - <module> - 314 - Namespace(adam_epsilon=1e-08, bert_dir='model_hub/chinese-bert-wwm-ext/', data_dir='./data/ske', dropout_prob=0.1, eval_batch_size=8, eval_steps=100, gpu_ids='0', log_dir='./logs/', lr=5e-05, max_grad_norm=1.0, max_seq_len=256, num_tags=49, other_lr=5e-05, output_dir='./checkpoints/', seed=123, train_batch_size=32, train_epochs=1, use_dev_num=1000, use_tensorboard='False', warmup_proportion=0.1, weight_decay=0.01)
2022-06-30 03:30:44,394 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['0']
2022-06-30 03:30:52,927 - INFO - train_utils.py - load_model_and_parallel - 87 - Load ckpt from ./checkpoints/bert/model.pt
2022-06-30 03:30:53,908 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['0']
2022-06-30 03:31:15,955 - INFO - main.py - <module> - 314 - Namespace(adam_epsilon=1e-08, bert_dir='model_hub/chinese-bert-wwm-ext/', data_dir='./data/ske', dropout_prob=0.1, eval_batch_size=8, eval_steps=100, gpu_ids='0', log_dir='./logs/', lr=5e-05, max_grad_norm=1.0, max_seq_len=256, num_tags=49, other_lr=5e-05, output_dir='./checkpoints/', seed=123, train_batch_size=32, train_epochs=1, use_dev_num=1000, use_tensorboard='False', warmup_proportion=0.1, weight_decay=0.01)
2022-06-30 03:31:22,841 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['0']
2022-06-30 03:31:31,424 - INFO - train_utils.py - load_model_and_parallel - 87 - Load ckpt from ./checkpoints/bert/model.pt
2022-06-30 03:31:32,384 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['0']
2022-06-30 03:33:07,563 - INFO - main.py - <module> - 316 - Namespace(adam_epsilon=1e-08, bert_dir='model_hub/chinese-bert-wwm-ext/', data_dir='./data/ske', dropout_prob=0.1, eval_batch_size=8, eval_steps=100, gpu_ids='0', log_dir='./logs/', lr=5e-05, max_grad_norm=1.0, max_seq_len=256, num_tags=49, other_lr=5e-05, output_dir='./checkpoints/', seed=123, train_batch_size=32, train_epochs=1, use_dev_num=1000, use_tensorboard='False', warmup_proportion=0.1, weight_decay=0.01)
2022-06-30 03:33:14,542 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['0']
2022-06-30 03:33:23,172 - INFO - train_utils.py - load_model_and_parallel - 87 - Load ckpt from ./checkpoints/bert/model.pt
2022-06-30 03:33:24,134 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['0']
2022-06-30 03:35:02,581 - INFO - main.py - <module> - 315 - Namespace(adam_epsilon=1e-08, bert_dir='model_hub/chinese-bert-wwm-ext/', data_dir='./data/ske', dropout_prob=0.1, eval_batch_size=8, eval_steps=100, gpu_ids='0', log_dir='./logs/', lr=5e-05, max_grad_norm=1.0, max_seq_len=256, num_tags=49, other_lr=5e-05, output_dir='./checkpoints/', seed=123, train_batch_size=32, train_epochs=1, use_dev_num=1000, use_tensorboard='False', warmup_proportion=0.1, weight_decay=0.01)
2022-06-30 03:35:09,602 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['0']
2022-06-30 03:35:18,845 - INFO - train_utils.py - load_model_and_parallel - 87 - Load ckpt from ./checkpoints/bert/model.pt
2022-06-30 03:35:19,804 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['0']
2022-06-30 03:36:22,396 - INFO - main.py - <module> - 315 - Namespace(adam_epsilon=1e-08, bert_dir='model_hub/chinese-bert-wwm-ext/', data_dir='./data/ske', dropout_prob=0.1, eval_batch_size=8, eval_steps=100, gpu_ids='0', log_dir='./logs/', lr=5e-05, max_grad_norm=1.0, max_seq_len=256, num_tags=49, other_lr=5e-05, output_dir='./checkpoints/', seed=123, train_batch_size=32, train_epochs=1, use_dev_num=1000, use_tensorboard='False', warmup_proportion=0.1, weight_decay=0.01)
2022-06-30 03:36:33,832 - INFO - main.py - <module> - 315 - Namespace(adam_epsilon=1e-08, bert_dir='model_hub/chinese-bert-wwm-ext/', data_dir='./data/ske', dropout_prob=0.1, eval_batch_size=8, eval_steps=10, gpu_ids='0', log_dir='./logs/', lr=5e-05, max_grad_norm=1.0, max_seq_len=256, num_tags=49, other_lr=5e-05, output_dir='./checkpoints/', seed=123, train_batch_size=32, train_epochs=1, use_dev_num=1000, use_tensorboard='False', warmup_proportion=0.1, weight_decay=0.01)
2022-06-30 03:36:40,826 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['0']
2022-06-30 03:36:48,812 - INFO - main.py - train - 64 - 【train】 epoch:0 0/5406 loss:269.4182 entity_loss:16.4784 head_loss:395.4211 tail_loss:396.3550
2022-06-30 03:36:50,498 - INFO - main.py - train - 64 - 【train】 epoch:0 1/5406 loss:270.2010 entity_loss:16.5278 head_loss:396.7143 tail_loss:397.3611
2022-06-30 03:36:52,189 - INFO - main.py - train - 64 - 【train】 epoch:0 2/5406 loss:266.3738 entity_loss:16.2139 head_loss:391.1121 tail_loss:391.7955
2022-06-30 03:36:53,879 - INFO - main.py - train - 64 - 【train】 epoch:0 3/5406 loss:269.4011 entity_loss:16.5687 head_loss:395.5172 tail_loss:396.1174
2022-06-30 03:36:55,569 - INFO - main.py - train - 64 - 【train】 epoch:0 4/5406 loss:250.0257 entity_loss:15.3507 head_loss:367.0298 tail_loss:367.6967
2022-06-30 03:36:57,281 - INFO - main.py - train - 64 - 【train】 epoch:0 5/5406 loss:259.6663 entity_loss:15.8807 head_loss:381.1756 tail_loss:381.9426
2022-06-30 03:36:58,986 - INFO - main.py - train - 64 - 【train】 epoch:0 6/5406 loss:256.6382 entity_loss:15.6712 head_loss:376.7637 tail_loss:377.4796
2022-06-30 03:37:00,694 - INFO - main.py - train - 64 - 【train】 epoch:0 7/5406 loss:271.7689 entity_loss:16.6315 head_loss:398.9182 tail_loss:399.7569
2022-06-30 03:37:02,421 - INFO - main.py - train - 64 - 【train】 epoch:0 8/5406 loss:258.0942 entity_loss:15.7037 head_loss:378.8968 tail_loss:379.6821
2022-06-30 03:37:04,130 - INFO - main.py - train - 64 - 【train】 epoch:0 9/5406 loss:258.2488 entity_loss:15.8331 head_loss:379.0878 tail_loss:379.8255
2022-06-30 03:48:08,606 - INFO - main.py - <module> - 315 - Namespace(adam_epsilon=1e-08, bert_dir='model_hub/chinese-bert-wwm-ext/', data_dir='./data/ske', dropout_prob=0.1, eval_batch_size=8, eval_steps=10, gpu_ids='0', log_dir='./logs/', lr=5e-05, max_grad_norm=1.0, max_seq_len=256, num_tags=49, other_lr=5e-05, output_dir='./checkpoints/', seed=123, train_batch_size=32, train_epochs=1, use_dev_num=1000, use_tensorboard='False', warmup_proportion=0.1, weight_decay=0.01)
2022-06-30 03:48:27,057 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['0']
2022-06-30 03:48:35,709 - INFO - train_utils.py - load_model_and_parallel - 87 - Load ckpt from ./checkpoints/bert/model.pt
2022-06-30 03:48:38,125 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['0']
2022-06-30 03:49:01,627 - INFO - train_utils.py - load_model_and_parallel - 87 - Load ckpt from ./checkpoints/bert/model.pt
2022-06-30 03:49:02,644 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['0']
2023-11-26 10:00:36,270 - INFO - main.py - <module> - 315 - Namespace(adam_epsilon=1e-08, bert_dir='./model_hub/bert-base-chinese/', data_dir='./data/ske', dropout_prob=0.1, eval_batch_size=12, eval_steps=32, gpu_ids='-1', log_dir='./logs/', lr=1e-05, max_grad_norm=1, max_seq_len=256, num_tags=49, other_lr=0.0003, output_dir='./checkpoints/', seed=123, train_batch_size=32, train_epochs=15, use_dev_num=32, use_tensorboard='True', warmup_proportion=0.1, weight_decay=0.01)
2023-11-26 10:01:42,656 - INFO - main.py - <module> - 315 - Namespace(adam_epsilon=1e-08, bert_dir='./model_hub/bert-base-chinese/', data_dir='./data/ske', dropout_prob=0.1, eval_batch_size=12, eval_steps=32, gpu_ids='-1', log_dir='./logs/', lr=1e-05, max_grad_norm=1, max_seq_len=256, num_tags=49, other_lr=0.0003, output_dir='./checkpoints/', seed=123, train_batch_size=32, train_epochs=15, use_dev_num=32, use_tensorboard='True', warmup_proportion=0.1, weight_decay=0.01)
2023-11-26 10:02:34,500 - INFO - main.py - <module> - 315 - Namespace(adam_epsilon=1e-08, bert_dir='./model_hub/bert-base-chinese/', data_dir='./data/ske', dropout_prob=0.1, eval_batch_size=12, eval_steps=32, gpu_ids='-1', log_dir='./logs/', lr=1e-05, max_grad_norm=1, max_seq_len=256, num_tags=49, other_lr=0.0003, output_dir='./checkpoints/', seed=123, train_batch_size=32, train_epochs=15, use_dev_num=32, use_tensorboard='True', warmup_proportion=0.1, weight_decay=0.01)
2023-11-26 10:02:36,025 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['-1']
2023-11-26 10:04:03,657 - INFO - main.py - <module> - 315 - Namespace(adam_epsilon=1e-08, bert_dir='./model_hub/bert-base-chinese/', data_dir='./data/ske', dropout_prob=0.1, eval_batch_size=12, eval_steps=32, gpu_ids='-1', log_dir='./logs/', lr=1e-05, max_grad_norm=1, max_seq_len=256, num_tags=49, other_lr=0.0003, output_dir='./checkpoints/', seed=123, train_batch_size=32, train_epochs=15, use_dev_num=32, use_tensorboard='True', warmup_proportion=0.1, weight_decay=0.01)
2023-11-26 10:04:05,111 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['-1']
2023-11-26 10:05:04,165 - INFO - main.py - train - 63 - 【train】 epoch:0 0/81090 loss:273.9406 entity_loss:16.8917 head_loss:402.0263 tail_loss:402.9039
2023-11-26 10:05:58,603 - INFO - main.py - train - 63 - 【train】 epoch:0 1/81090 loss:256.4917 entity_loss:15.7166 head_loss:376.3349 tail_loss:377.4236
2023-11-26 10:20:57,587 - INFO - main.py - <module> - 315 - Namespace(adam_epsilon=1e-08, bert_dir='./model_hub/bert-base-chinese/', data_dir='./data/ske', dropout_prob=0.1, eval_batch_size=12, eval_steps=32, gpu_ids='-1', log_dir='./logs/', lr=1e-05, max_grad_norm=1, max_seq_len=256, num_tags=49, other_lr=0.0003, output_dir='./checkpoints/', seed=123, train_batch_size=32, train_epochs=15, use_dev_num=32, use_tensorboard='True', warmup_proportion=0.1, weight_decay=0.01)
2023-11-26 10:20:59,124 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['-1']
2023-11-26 17:42:01,847 - INFO - main.py - <module> - 321 - Namespace(adam_epsilon=1e-08, bert_dir='./model_hub/bert-base-chinese/', data_dir='./data/ske', dropout_prob=0.1, eval_batch_size=12, eval_steps=32, gpu_ids='-1', log_dir='./logs/', lr=1e-05, max_grad_norm=1, max_seq_len=256, num_tags=49, other_lr=0.0003, output_dir='./checkpoints/', seed=123, train_batch_size=32, train_epochs=15, use_dev_num=32, use_tensorboard='True', warmup_proportion=0.1, weight_decay=0.01)
2023-11-26 17:42:03,378 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['-1']
2023-11-26 17:43:00,177 - INFO - main.py - train - 67 - 【train】 epoch:0 0/81090 loss:274.1717 entity_loss:16.9058 head_loss:402.2278 tail_loss:403.3814 speed: 51.29ms/b
2023-11-26 17:43:47,383 - INFO - main.py - train - 67 - 【train】 epoch:0 1/81090 loss:256.6770 entity_loss:15.7209 head_loss:376.5012 tail_loss:377.8088 speed: 47.20ms/b
2023-11-26 17:44:49,868 - INFO - main.py - train - 67 - 【train】 epoch:0 2/81090 loss:269.0973 entity_loss:16.5296 head_loss:394.7513 tail_loss:396.0110 speed: 62.48ms/b
2023-11-26 17:45:58,481 - INFO - main.py - train - 67 - 【train】 epoch:0 3/81090 loss:263.5986 entity_loss:16.1610 head_loss:386.6591 tail_loss:387.9758 speed: 68.61ms/b
2023-11-26 17:47:01,620 - INFO - main.py - train - 67 - 【train】 epoch:0 4/81090 loss:262.0669 entity_loss:16.1909 head_loss:384.3964 tail_loss:385.6134 speed: 63.14ms/b
2023-11-26 17:47:59,652 - INFO - main.py - train - 67 - 【train】 epoch:0 5/81090 loss:265.4700 entity_loss:16.4846 head_loss:389.3178 tail_loss:390.6075 speed: 58.03ms/b
2023-11-26 17:48:48,547 - INFO - main.py - train - 67 - 【train】 epoch:0 6/81090 loss:254.6439 entity_loss:15.5490 head_loss:373.5656 tail_loss:374.8171 speed: 48.89ms/b
2023-11-26 17:49:44,147 - INFO - main.py - train - 67 - 【train】 epoch:0 7/81090 loss:259.0671 entity_loss:15.8560 head_loss:379.9945 tail_loss:381.3508 speed: 55.60ms/b
2023-11-26 17:50:35,145 - INFO - main.py - train - 67 - 【train】 epoch:0 8/81090 loss:259.5800 entity_loss:15.9702 head_loss:380.7346 tail_loss:382.0353 speed: 51.00ms/b
2023-11-26 17:51:23,499 - INFO - main.py - train - 67 - 【train】 epoch:0 9/81090 loss:260.5485 entity_loss:16.0177 head_loss:382.0992 tail_loss:383.5284 speed: 48.35ms/b
2023-11-26 17:52:14,639 - INFO - main.py - train - 67 - 【train】 epoch:0 10/81090 loss:274.8178 entity_loss:16.7167 head_loss:403.2352 tail_loss:404.5015 speed: 51.14ms/b
2023-11-26 17:53:03,080 - INFO - main.py - train - 67 - 【train】 epoch:0 11/81090 loss:257.0414 entity_loss:15.8750 head_loss:377.0292 tail_loss:378.2200 speed: 48.44ms/b
2023-11-26 17:54:07,129 - INFO - main.py - train - 67 - 【train】 epoch:0 12/81090 loss:276.8197 entity_loss:17.0266 head_loss:406.1174 tail_loss:407.3150 speed: 64.05ms/b
2023-11-26 17:55:09,506 - INFO - main.py - train - 67 - 【train】 epoch:0 13/81090 loss:250.9238 entity_loss:15.3782 head_loss:368.0872 tail_loss:369.3059 speed: 62.37ms/b
2023-11-26 17:56:20,630 - INFO - main.py - train - 67 - 【train】 epoch:0 14/81090 loss:256.5724 entity_loss:15.7474 head_loss:376.3469 tail_loss:377.6230 speed: 71.12ms/b
2023-11-26 17:57:17,139 - INFO - main.py - train - 67 - 【train】 epoch:0 15/81090 loss:274.5144 entity_loss:16.8895 head_loss:402.6678 tail_loss:403.9861 speed: 56.51ms/b
2023-11-26 17:58:09,946 - INFO - main.py - train - 67 - 【train】 epoch:0 16/81090 loss:275.9294 entity_loss:16.9678 head_loss:404.6689 tail_loss:406.1515 speed: 52.80ms/b
2023-11-26 18:43:29,246 - INFO - main.py - <module> - 321 - Namespace(adam_epsilon=1e-08, bert_dir='./model_hub/bert-base-chinese/', data_dir='./data/ske', dropout_prob=0.1, eval_batch_size=12, eval_steps=32, gpu_ids='-1', log_dir='./logs/', lr=1e-05, max_grad_norm=1, max_seq_len=256, num_tags=49, other_lr=0.0003, output_dir='./checkpoints/', seed=123, train_batch_size=32, train_epochs=15, use_dev_num=32, use_tensorboard='True', warmup_proportion=0.1, weight_decay=0.01)
2023-11-26 18:43:30,953 - INFO - train_utils.py - load_model_and_parallel - 97 - Use single gpu in: ['-1']
2023-11-26 18:44:28,277 - INFO - main.py - train - 67 - 【train】 epoch:0 0/81090 loss:274.1717 entity_loss:16.9058 head_loss:402.2278 tail_loss:403.3814 speed: 51.75ms/b
2023-11-26 18:45:18,246 - INFO - main.py - train - 67 - 【train】 epoch:0 1/81090 loss:256.6770 entity_loss:15.7209 head_loss:376.5012 tail_loss:377.8088 speed: 49.97ms/b
2023-11-26 18:46:10,526 - INFO - main.py - train - 67 - 【train】 epoch:0 2/81090 loss:269.0973 entity_loss:16.5296 head_loss:394.7513 tail_loss:396.0110 speed: 52.28ms/b
2023-11-26 18:47:02,438 - INFO - main.py - train - 67 - 【train】 epoch:0 3/81090 loss:263.5986 entity_loss:16.1610 head_loss:386.6591 tail_loss:387.9758 speed: 51.91ms/b
2023-11-26 18:47:52,230 - INFO - main.py - train - 67 - 【train】 epoch:0 4/81090 loss:262.0669 entity_loss:16.1909 head_loss:384.3964 tail_loss:385.6134 speed: 49.79ms/b
2023-11-26 18:48:36,109 - INFO - main.py - train - 67 - 【train】 epoch:0 5/81090 loss:265.4700 entity_loss:16.4846 head_loss:389.3178 tail_loss:390.6075 speed: 43.88ms/b
2023-11-26 18:49:20,000 - INFO - main.py - train - 67 - 【train】 epoch:0 6/81090 loss:254.6439 entity_loss:15.5490 head_loss:373.5656 tail_loss:374.8171 speed: 43.89ms/b
2023-11-26 18:50:05,097 - INFO - main.py - train - 67 - 【train】 epoch:0 7/81090 loss:259.0671 entity_loss:15.8560 head_loss:379.9945 tail_loss:381.3508 speed: 45.09ms/b
2023-11-26 18:50:59,102 - INFO - main.py - train - 67 - 【train】 epoch:0 8/81090 loss:259.5800 entity_loss:15.9702 head_loss:380.7346 tail_loss:382.0353 speed: 54.00ms/b
2023-11-26 18:51:47,199 - INFO - main.py - train - 67 - 【train】 epoch:0 9/81090 loss:260.5485 entity_loss:16.0177 head_loss:382.0992 tail_loss:383.5284 speed: 48.09ms/b
2023-11-26 18:52:36,121 - INFO - main.py - train - 67 - 【train】 epoch:0 10/81090 loss:274.8178 entity_loss:16.7167 head_loss:403.2352 tail_loss:404.5015 speed: 48.92ms/b
2023-11-26 18:53:23,533 - INFO - main.py - train - 67 - 【train】 epoch:0 11/81090 loss:257.0414 entity_loss:15.8750 head_loss:377.0292 tail_loss:378.2200 speed: 47.41ms/b
2023-11-26 18:54:13,078 - INFO - main.py - train - 67 - 【train】 epoch:0 12/81090 loss:276.8197 entity_loss:17.0266 head_loss:406.1174 tail_loss:407.3150 speed: 49.54ms/b
2023-11-26 18:55:00,639 - INFO - main.py - train - 67 - 【train】 epoch:0 13/81090 loss:250.9238 entity_loss:15.3782 head_loss:368.0872 tail_loss:369.3059 speed: 47.56ms/b
2023-11-26 18:55:44,341 - INFO - main.py - train - 67 - 【train】 epoch:0 14/81090 loss:256.5724 entity_loss:15.7474 head_loss:376.3469 tail_loss:377.6230 speed: 43.70ms/b
2023-11-26 18:56:32,555 - INFO - main.py - train - 67 - 【train】 epoch:0 15/81090 loss:274.5144 entity_loss:16.8895 head_loss:402.6678 tail_loss:403.9861 speed: 48.21ms/b
2023-11-26 18:57:18,970 - INFO - main.py - train - 67 - 【train】 epoch:0 16/81090 loss:275.9294 entity_loss:16.9678 head_loss:404.6689 tail_loss:406.1515 speed: 46.41ms/b
2023-11-26 18:58:02,757 - INFO - main.py - train - 67 - 【train】 epoch:0 17/81090 loss:255.2874 entity_loss:15.6574 head_loss:374.4333 tail_loss:375.7714 speed: 43.78ms/b
2023-11-26 18:58:53,557 - INFO - main.py - train - 67 - 【train】 epoch:0 18/81090 loss:260.5949 entity_loss:16.1022 head_loss:382.1034 tail_loss:383.5793 speed: 50.80ms/b
2023-11-26 18:59:42,242 - INFO - main.py - train - 67 - 【train】 epoch:0 19/81090 loss:274.1765 entity_loss:16.8813 head_loss:402.2016 tail_loss:403.4466 speed: 48.68ms/b
2023-11-26 19:00:26,466 - INFO - main.py - train - 67 - 【train】 epoch:0 20/81090 loss:257.5312 entity_loss:15.7964 head_loss:377.8659 tail_loss:378.9315 speed: 44.22ms/b
2023-11-26 19:01:14,890 - INFO - main.py - train - 67 - 【train】 epoch:0 21/81090 loss:266.4056 entity_loss:16.3681 head_loss:390.7372 tail_loss:392.1115 speed: 48.42ms/b
2023-11-26 19:02:05,608 - INFO - main.py - train - 67 - 【train】 epoch:0 22/81090 loss:252.1879 entity_loss:15.5281 head_loss:369.8932 tail_loss:371.1424 speed: 50.72ms/b
